import re
import subprocess
from datetime import datetime, timedelta
from datadog import initialize, statsd

# Initialize DogStatsD
initialize(statsd_host='localhost', statsd_port=8125)

# Configuration
LOG_FILE = 'LOG.txt'
DATE_FMT = '%d-%m-%y'

def get_dates():
    """Get current date and previous date in required format"""
    current_date = datetime.now()
    prev_date = current_date - timedelta(days=1)
    return {
        'current': current_date.strftime(DATE_FMT),
        'previous': prev_date.strftime(DATE_FMT)
    }

def grep_log(pattern):
    """Use grep to efficiently search large log files"""
    try:
        process = subprocess.Popen(
            ['grep', '-a', '-E', pattern, LOG_FILE],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            universal_newlines=True
        )
        stdout, stderr = process.communicate()
        return stdout.splitlines()
    except Exception as e:
        print(f"Error running grep: {str(e)}")
        return []

def extract_timings():
    """Extract timings using grep and regex"""
    dates = get_dates()
    metrics = {}

    # Patterns to search for each metric
    search_patterns = {
        'eod_bod': {
            'start': {
                'grep_pattern': 'SC195 195 START',
                'regex': r'SC195 195 START (\d{2}-\d{2}-\d{2}) (\d{2}:\d{2}:\d{2})',
                'target_date': dates['previous']
            },
            'end': {
                'grep_pattern': 'PPBOV 295 START',
                'regex': r'PPBOV 295 START (\d{2}-\d{2}-\d{2}) (\d{2}:\d{2}:\d{2})',
                'target_date': dates['current']
            }
        },
        'ach_300': {
            'start': {
                'grep_pattern': 'PEPI3 START',
                'regex': r'PEPI3 START (\d{2}-\d{2}-\d{2}) (\d{2}:\d{2}:\d{2})',
                'target_date': dates['current']
            },
            'end': {
                'grep_pattern': 'ACHSV 295 END',
                'regex': r'ACHSV 295 END (\d{2}-\d{2}-\d{2}) (\d{2}:\d{2}:\d{2})',
                'target_date': dates['current']
            }
        },
        'ach_700': {
            'start': {
                'grep_pattern': 'PIAB7 START',
                'regex': r'PIAB7 START (\d{2}-\d{2}-\d{2}) (\d{2}:\d{2}:\d{2})',
                'target_date': dates['current']
            },
            'end': {
                'grep_pattern': 'ACHFV 295 END',
                'regex': r'ACHFV 295 END (\d{2}-\d{2}-\d{2}) (\d{2}:\d{2}:\d{2})',
                'target_date': dates['current']
            }
        }
    }

    for batch_name, batch_config in search_patterns.items():
        metrics[batch_name] = {}
        
        for timing_type, config in batch_config.items():
            # Initialize with empty values
            metrics[batch_name][timing_type] = {
                'datetime_str': '',
                'found': False
            }
            
            # First use grep to find relevant lines quickly
            lines = grep_log(config['grep_pattern'])
            
            # Then apply precise regex to matching lines
            for line in lines:
                match = re.search(config['regex'], line)
                if match:
                    date_str, time_str = match.groups()
                    if date_str == config['target_date']:
                        metrics[batch_name][timing_type] = {
                            'datetime_str': f"{date_str} {time_str}",
                            'found': True
                        }
                        break  # Only need the first match per metric

    return metrics

def datetime_to_unix(datetime_str, date_fmt="%d-%m-%y %H:%M:%S"):
    """Convert datetime string to Unix timestamp"""
    try:
        dt = datetime.strptime(datetime_str, date_fmt)
        return int(dt.timestamp())
    except:
        return 0  # Return 0 if conversion fails

def send_to_datadog(metrics):
    """Send metrics to Datadog with Unix timestamps as values"""
    for batch_name, timings in metrics.items():
        try:
            # Send start time metric
            if timings['start']['found']:
                unix_ts = datetime_to_unix(timings['start']['datetime_str'])
                statsd.gauge(
                    f'batch_processing.{batch_name}.start',
                    value=unix_ts,
                    tags=[f'batch:{batch_name}']
                )
            
            # Send end time metric
            if timings['end']['found']:
                unix_ts = datetime_to_unix(timings['end']['datetime_str'])
                statsd.gauge(
                    f'batch_processing.{batch_name}.end',
                    value=unix_ts,
                    tags=[f'batch:{batch_name}']
                )
            
            # Calculate and send duration if both timings exist
            if timings['start']['found'] and timings['end']['found']:
                start_ts = datetime_to_unix(timings['start']['datetime_str'])
                end_ts = datetime_to_unix(timings['end']['datetime_str'])
                duration = end_ts - start_ts
                statsd.gauge(
                    f'batch_processing.{batch_name}.duration',
                    value=duration,
                    tags=[f'batch:{batch_name}']
                )
                
        except Exception as e:
            print(f"Error processing {batch_name}: {str(e)}")

if __name__ == "__main__":
    print("Processing log file...")
    metrics = extract_timings()
    
    print("\nMetrics extracted:")
    for batch, timings in metrics.items():
        print(f"\n{batch}:")
        for timing_type, data in timings.items():
            status = data['datetime_str'] if data['found'] else "Not found"
            print(f"  {timing_type}: {status}")
    
    print("\nSending to Datadog...")
    send_to_datadog(metrics)
    print("Done!")
